{"cells":[{"cell_type":"markdown","source":["# Land-use classification using Semantic Segmentation\n","Johnathan Clementi & Gianluca Mangiapane   \n","Remote Sensing: MUSA 650, Spring 2022   \n","\n","### Problem:\n","In image analysis, traditional classification methods return a single output label for each image. In the context of land-use classification from satelitte/aerial imagery, a traditional classification model would only return a single label for an image that has many land-use types within it. However, there are often many different types of land-use held within a single satelitte or aerial image. An issue can arise where if an image has 51% of a certain type of land use, the classifier will assign that image a single label. The remaining 49% of land-use data that doesn't fall under the single-classification is then hidden from the data user, which could be useful or valubale information. Thus, we hope to solve this problem through utilizing semantic segmentation, or the classification of individual pixels (and therefore all the different classes) within an image. \n","\n","### Methods \n","\n","The primary method that we aim to use is that of Semantic Segmentation, a form of dense prediction where every pixel in an image is given a label of a corresponding class. Semantic Segmentation has become prevalent in both the medical field as a way of identifying harmful cells from healthy cells in brain and organ scans, and autonomous driving cars being able to idenfity free road spcae from other vehicles, pedestrians, and road signs. However, there has also been a growing application of Semantic Segmentation to the field of Remote Sensing, and identifying different classes of land use and objects from satellite imagery. \n","\n","Our segmentation method will consist of applying a U-NET on our image dataset. A U-NET initially follows a general CNN architecture of convolutional and pooling layers, but it starts with encoding the input image down to a simplied features map, and then decodes that map back up to the input image, through deconvolutional layers, and therefore becomes a fully convolutional network. A defining feature of the U-net is that at every up-sampling of the decoder layer (the upwards of the U), information is sent from its respective down-sampling of the encoder layer. The encoder layer has more defined information, and therefore helps the decoder layer have more accurate outputs. \n","\n","Considering Image Segmentation is a very complicated process in and of itself, if we run into difficulties with the image dataset's current form, one back up proposal is that we can split individual images into squares of equal size, and decrease the number of different classes in each sqaure. From there, we can perform a more mangeable multi-classification approach, such as a traditional CNN with convulutional and pooling layers instead of a U-NET. \n","\n","### Datasets to use \n","\n","We plan on using Earth Imagery collected by the DigitialGlobe satellite in 2018, and presented as a challenge to \"Parse the Earth through Satellite Images\" by Cornell University. The dataset was also provided on Kaggle. \n","\n","  -Challenge Information \n","  https://arxiv.org/abs/1805.06561 \n","\n","  -Kaggle Link\n","  https://www.kaggle.com/datasets/balraj98/deepglobe-land-cover-classification-dataset \n","\n","\n","\n","The training data contains 803 satellite imagery in RGB, size 2448 x 2448, with pixel resolution of 50cm. The dataset also contains 171 validation and 172 test images, with no masks. In the training set, each satellite image has a RGB image with 7 classes of labels, composed of Urban (0,255,255), Agriculture land (255,255,0), Rangeland (255,0,255), Forest land (0,255,0), Water (0,0,255), Barren land (255,255,255), and Unknown (0,0,0) i.e., clouds and others. \n","\n","Once we have our project model trained to an acceptable level of performance, an additional goal we have is to use our model on the Microsoft Chesapeake NAIP imagery to see if our model can segmentic land types in a real world application. \n","\n","https://mlhub.earth/data/microsoft_chesapeake \n","\n","\n","\n","### Background information Sources\n","\n","https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47 \n","\n","https://nanonets.com/blog/semantic-image-segmentation-2020/ \n","\n","https://keras.io/examples/vision/oxford_pets_image_segmentation/ \n"],"metadata":{"id":"ieCGC_Kt4Nzn"}},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ii6Th2X9Z3aq","executionInfo":{"status":"ok","timestamp":1649253016716,"user_tz":240,"elapsed":10169,"user":{"displayName":"John Clementi","userId":"03730803467724976514"}},"outputId":"ecc269ab-06fc-4d2e-c41b-490b879e195d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.24.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.5)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (13.0.0)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n","Installing collected packages: tf-estimator-nightly\n","Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"]}]},{"cell_type":"code","source":["import os, cv2\n","import numpy as np\n","import pandas as pd\n","import random, tqdm\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import albumentations as album"],"metadata":{"id":"BSZolTpNSYy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zbCI48tUELSj"},"outputs":[],"source":["## import packages\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential, Model\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Activation, Dropout, Flatten\n","from tensorflow.keras.layers import BatchNormalization\n","\n","from tensorflow.keras.optimizers import Adam\n","from keras.callbacks import LearningRateScheduler\n","from keras.regularizers import l2, l1\n","import math"]},{"cell_type":"code","source":["## Mount drive folder\n","from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aOZe9WhsGiRm","executionInfo":{"status":"ok","timestamp":1649253051329,"user_tz":240,"elapsed":17474,"user":{"displayName":"John Clementi","userId":"03730803467724976514"}},"outputId":"c113ba0a-097e-4fb3-ac9d-5ae1b9a947c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["## Create path to data\n","# Gianluca's Path:\n","# path = \"/content/drive/MyDrive/FinalPrj/data\"\n","\n","# Johnathan's Path:\n","path = \"/content/drive/MyDrive/Grad School/Penn_MUSA/Spring2022/650_RemoteSensing/FinalPrj/data\"\n","\n","## Classes and their respective pixel values\n","class_dict = pd.read_csv('{}/class_dict.csv'.format(path))\n","class_names = class_dict['name'].values.tolist()\n","class_rgb_vals = class_dict[['r', 'g', 'b']].values.tolist()\n","\n","metadata = pd.read_csv('{}/metadata.csv'.format(path))"],"metadata":{"id":"g7CZHErFGkSQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metadata.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"ouFe26FgH0mU","executionInfo":{"status":"ok","timestamp":1649205209995,"user_tz":240,"elapsed":289,"user":{"displayName":"Gianluca Mangiapane","userId":"02429346958353056946"}},"outputId":"14978399-87df-4f40-d318-b69eec7f0d97"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   image_id  split        sat_image_path              mask_path\n","0    100694  train  train/100694_sat.jpg  train/100694_mask.png\n","1    102122  train  train/102122_sat.jpg  train/102122_mask.png\n","2     10233  train   train/10233_sat.jpg   train/10233_mask.png\n","3    103665  train  train/103665_sat.jpg  train/103665_mask.png\n","4    103730  train  train/103730_sat.jpg  train/103730_mask.png"],"text/html":["\n","  <div id=\"df-e1b14c62-1a6d-4695-8211-9cdc8c8c3910\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>split</th>\n","      <th>sat_image_path</th>\n","      <th>mask_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>100694</td>\n","      <td>train</td>\n","      <td>train/100694_sat.jpg</td>\n","      <td>train/100694_mask.png</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>102122</td>\n","      <td>train</td>\n","      <td>train/102122_sat.jpg</td>\n","      <td>train/102122_mask.png</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10233</td>\n","      <td>train</td>\n","      <td>train/10233_sat.jpg</td>\n","      <td>train/10233_mask.png</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>103665</td>\n","      <td>train</td>\n","      <td>train/103665_sat.jpg</td>\n","      <td>train/103665_mask.png</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>103730</td>\n","      <td>train</td>\n","      <td>train/103730_sat.jpg</td>\n","      <td>train/103730_mask.png</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1b14c62-1a6d-4695-8211-9cdc8c8c3910')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e1b14c62-1a6d-4695-8211-9cdc8c8c3910 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e1b14c62-1a6d-4695-8211-9cdc8c8c3910');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["class_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVY1ZAIzPY10","executionInfo":{"status":"ok","timestamp":1649205128767,"user_tz":240,"elapsed":360,"user":{"displayName":"Gianluca Mangiapane","userId":"02429346958353056946"}},"outputId":"c3172326-6f4a-4cb4-a187-afcba1fc9759"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['urban_land',\n"," 'agriculture_land',\n"," 'rangeland',\n"," 'forest_land',\n"," 'water',\n"," 'barren_land',\n"," 'unknown']"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["class_rgb_vals"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvQ_0FFTPbME","executionInfo":{"status":"ok","timestamp":1649205130098,"user_tz":240,"elapsed":2,"user":{"displayName":"Gianluca Mangiapane","userId":"02429346958353056946"}},"outputId":"7826d7b0-e445-4f0d-bcfc-944aff0203b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0, 255, 255],\n"," [255, 255, 0],\n"," [255, 0, 255],\n"," [0, 255, 0],\n"," [0, 0, 255],\n"," [255, 255, 255],\n"," [0, 0, 0]]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["metadata = pd.read_csv(os.path.join(path, 'metadata.csv'))\n","metadata = metadata[metadata['split']=='train']\n","metadata = metadata[['image_id', 'sat_image_path', 'mask_path']]\n","metadata['sat_image_path'] = metadata['sat_image_path'].apply(lambda img_pth: os.path.join(path, img_pth))\n","metadata['mask_path'] = metadata['mask_path'].apply(lambda img_pth: os.path.join(path, img_pth))\n","# Shuffle DataFrame\n","metadata = metadata.sample(frac=1).reset_index(drop=True)\n","\n","# Perform 90/10 split for train / val\n","valid_df = metadata.sample(frac=0.1, random_state=42)\n","train_df = metadata.drop(valid_df.index)\n","len(train_df), len(valid_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXftjR_aSL2m","executionInfo":{"status":"ok","timestamp":1649205543951,"user_tz":240,"elapsed":298,"user":{"displayName":"Gianluca Mangiapane","userId":"02429346958353056946"}},"outputId":"4efefa19-5be6-4a24-91fb-7cf7d8f70731"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(723, 80)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["class_dict = pd.read_csv(os.path.join(path, 'class_dict.csv'))\n","# Get class names\n","class_names = class_dict['name'].tolist()\n","# Get class RGB values\n","class_rgb_values = class_dict[['r','g','b']].values.tolist()\n","\n","# Useful to shortlist specific classes in datasets with large number of classes\n","select_classes = class_names\n","\n","# Get RGB values of required classes\n","select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n","select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n","\n","print('Selected classes and their corresponding RGB values in labels:')\n","print('Class Names: ', class_names)\n","print('Class RGB values: ', class_rgb_values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AmzW3drfSL5S","executionInfo":{"status":"ok","timestamp":1649205579485,"user_tz":240,"elapsed":766,"user":{"displayName":"Gianluca Mangiapane","userId":"02429346958353056946"}},"outputId":"91495663-17ab-49db-df89-8cc0b7988984"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Selected classes and their corresponding RGB values in labels:\n","Class Names:  ['urban_land', 'agriculture_land', 'rangeland', 'forest_land', 'water', 'barren_land', 'unknown']\n","Class RGB values:  [[0, 255, 255], [255, 255, 0], [255, 0, 255], [0, 255, 0], [0, 0, 255], [255, 255, 255], [0, 0, 0]]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"EIWiTWE7SL73"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"1e7e171cea119ef928e07b5fcb87425f253860bf90537fd64366dbf2948eb0fb"},"kernelspec":{"display_name":"Python 3.9.10 ('musa650_HW2')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.10"},"orig_nbformat":4,"colab":{"name":"landCov_semSeg.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}